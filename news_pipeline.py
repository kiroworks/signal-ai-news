"""
SIGNAL AI News Pipeline
========================
è‡ªå‹•ã§AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›† â†’ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â†’ æ—¥è‹±è¨˜äº‹ç”Ÿæˆ â†’ DBä¿å­˜(draft) â†’ XæŠ•ç¨¿

å¿…è¦ãªç’°å¢ƒå¤‰æ•°ï¼ˆGitHub Secretsï¼‰:
  ANTHROPIC_API_KEY = Claude APIã‚­ãƒ¼
  SUPABASE_URL      = Supabaseãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆURL
  SUPABASE_KEY      = Supabase service_role keyï¼ˆSecret keyï¼‰

ä»»æ„ï¼ˆXé€£æºï¼‰:
  TWITTER_API_KEY / TWITTER_API_SECRET / TWITTER_ACCESS_TOKEN / TWITTER_ACCESS_SECRET

å®Ÿè¡Œ:
  pip install anthropic feedparser supabase tweepy python-dotenv
  python news_pipeline.py
"""

import os
import re
import json
import hashlib
import logging
from datetime import datetime, timezone
from typing import Optional
import feedparser
import anthropic
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
log = logging.getLogger("signal")

# ============================================================
# CONFIG
# ============================================================

# "draft"   = Supabaseã§ç¢ºèªå¾Œã«æ‰‹å‹•ã§ published ã«å¤‰æ›´ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰
# "published" = å³æ™‚å…¬é–‹ï¼ˆå…¨è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ï¼‰
DEFAULT_STATUS = "draft"

RSS_SOURCES = [
    {"url": "https://arxiv.org/rss/cs.AI",            "source": "arXiv AI",        "category": "research", "trust": 95},
    {"url": "https://arxiv.org/rss/cs.LG",            "source": "arXiv ML",        "category": "research", "trust": 95},
    {"url": "https://deepmind.google/blog/rss.xml",   "source": "DeepMind",        "category": "research", "trust": 98},
    {"url": "https://openai.com/blog/rss.xml",        "source": "OpenAI",          "category": "product",  "trust": 99},
    {"url": "https://www.anthropic.com/rss.xml",      "source": "Anthropic",       "category": "product",  "trust": 99},
    {"url": "https://ai.google/blog/rss",             "source": "Google AI",       "category": "product",  "trust": 97},
    {"url": "https://ai.meta.com/blog/rss",           "source": "Meta AI",         "category": "product",  "trust": 96},
    {"url": "https://mistral.ai/news/rss.xml",        "source": "Mistral AI",      "category": "product",  "trust": 92},
    {"url": "https://techcrunch.com/tag/artificial-intelligence/feed/",
                                                      "source": "TechCrunch",      "category": "business", "trust": 80},
    {"url": "https://www.technologyreview.com/feed/", "source": "MIT Tech Review", "category": "research", "trust": 90},
    {"url": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
                                                      "source": "The Verge",       "category": "product",  "trust": 78},
    {"url": "https://artificialintelligenceact.eu/feed/",
                                                      "source": "EU AI Act",       "category": "policy",   "trust": 95},
    {"url": "https://www.nist.gov/artificial-intelligence/rss.xml",
                                                      "source": "NIST",            "category": "policy",   "trust": 97},
]

SCORING_PROMPT = """
ã‚ãªãŸã¯AIå°‚é–€ã®ã‚¸ãƒ£ãƒ¼ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è¨˜äº‹ã‚’è©•ä¾¡ãƒ»ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘{title}
ã€æœ¬æ–‡è¦ç´„ã€‘{summary}
ã€ã‚½ãƒ¼ã‚¹ã€‘{source} (ä¿¡é ¼åº¦: {trust}/100)
ã€ã‚«ãƒ†ã‚´ãƒªã€‘{category}

ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–ï¼ˆåˆè¨ˆ100ç‚¹ï¼‰:
- æ–°è¦æ€§ 30ç‚¹: æ–°ã—ã„ç™ºè¦‹ãƒ»ç™ºè¡¨ã‹ï¼ˆæ—¢çŸ¥æƒ…å ±ã¯ä½ã‚¹ã‚³ã‚¢ï¼‰
- ä¿¡é ¼æ€§ 25ç‚¹: ä¸€æ¬¡ã‚½ãƒ¼ã‚¹ãƒ»å…¬å¼ç™ºè¡¨ãƒ»æŸ»èª­æ¸ˆã¿ã‹
- é‡è¦æ€§ 25ç‚¹: AIåˆ†é‡å…¨ä½“ã«å½±éŸ¿ã™ã‚‹ã‹
- å®Ÿç”¨æ€§ 20ç‚¹: é–‹ç™ºè€…ãƒ»ç ”ç©¶è€…ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã«æœ‰ç›Šã‹

é‡è¦åº¦åˆ†é¡:
- 90ç‚¹ä»¥ä¸Š = critical
- 75ã€œ89ç‚¹ = high
- 60ã€œ74ç‚¹ = normal
- 60ç‚¹æœªæº€ = skipï¼ˆæ²è¼‰ã—ãªã„ï¼‰

JSONã®ã¿ã§å›ç­”ï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ä¸è¦ï¼‰:
{{
  "score": <æ•´æ•°>,
  "importance": <"critical"|"high"|"normal"|"skip">,
  "title_ja": <æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ30å­—ä»¥å†…ï¼‰>,
  "title_en": <è‹±èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ10èªä»¥å†…ï¼‰>,
  "summary_ja": <æ—¥æœ¬èªè¦ç´„ï¼ˆ150å­—ç¨‹åº¦ï¼‰>,
  "summary_en": <English summary (around 80 words)>,
  "key_insight": <ãªãœã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒé‡è¦ã‹ãƒ»æ¥­ç•Œã¸ã®å…·ä½“çš„ãªå½±éŸ¿ï¼ˆæ—¥æœ¬èª2æ–‡ï¼‰>,
  "tags": <ã‚¿ã‚°é…åˆ— ä¾‹: ["LLM", "OpenAI", "Benchmark"]>
}}
"""

# ============================================================
# UTILITIES
# ============================================================

def strip_html(text: str) -> str:
    text = re.sub(r'<[^>]+>', '', text)
    text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>') \
               .replace('&quot;', '"').replace('&#39;', "'").replace('&nbsp;', ' ')
    return re.sub(r'\s+', ' ', text).strip()

def parse_json_safe(raw: str) -> dict:
    """Claude ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ JSON ã‚’å®‰å…¨ã«ãƒ‘ãƒ¼ã‚¹"""
    if "```" in raw:
        parts = raw.split("```")
        raw = parts[1].replace("json", "", 1).strip() if len(parts) > 1 else raw
    start, end = raw.find("{"), raw.rfind("}") + 1
    if start == -1 or end == 0:
        raise ValueError("JSON not found")
    return json.loads(raw[start:end])

# ============================================================
# STEP 0: æ—¢å­˜è¨˜äº‹ã‚’å–å¾—ï¼ˆAPIã‚³ã‚¹ãƒˆå‰Šæ¸›ã®æ ¸å¿ƒï¼‰
# ============================================================

def get_existing_ids(supabase_url: str, supabase_key: str) -> set:
    """
    DBå†…ã®æ—¢å­˜è¨˜äº‹IDã‚’å–å¾—ã€‚
    ãƒ•ã‚§ãƒƒãƒã—ãŸè¨˜äº‹ãŒã™ã§ã«DBã«ã‚ã‚Œã°Claudeã‚’å‘¼ã°ãšã‚¹ã‚­ãƒƒãƒ—ã€‚
    2å›ç›®ä»¥é™ã®å®Ÿè¡Œã§APIã‚³ã‚¹ãƒˆã‚’å¤§å¹…å‰Šæ¸›ï¼ˆå¹³å‡80%æ¸›ï¼‰ã€‚
    """
    if not supabase_url or not supabase_key:
        return set()
    try:
        sb = create_client(supabase_url, supabase_key)
        result = sb.table("articles").select("id").execute()
        ids = {row["id"] for row in (result.data or [])}
        log.info(f"Existing articles in DB: {len(ids)}")
        return ids
    except Exception as e:
        log.warning(f"Could not fetch existing IDs: {e}")
        return set()

# ============================================================
# STEP 1: RSSåé›†
# ============================================================

def fetch_feeds() -> list[dict]:
    articles = []
    seen_urls = set()

    for source in RSS_SOURCES:
        try:
            feed = feedparser.parse(source["url"])
            count = 0
            for entry in feed.entries[:5]:
                url = entry.get("link", "")
                if not url or url in seen_urls:
                    continue
                seen_urls.add(url)

                article_id = hashlib.md5(url.encode()).hexdigest()
                articles.append({
                    "id":           article_id,
                    "title":        strip_html(entry.get("title", "")),
                    "summary":      strip_html(entry.get("summary", entry.get("description", "")))[:2000],
                    "url":          url,
                    "source":       source["source"],
                    "source_trust": source["trust"],
                    "category":     source["category"],
                })
                count += 1
            log.info(f"âœ“ {source['source']}: {count} entries")
        except Exception as e:
            log.warning(f"âœ— {source['source']}: {e}")

    log.info(f"Total fetched: {len(articles)}")
    return articles

# ============================================================
# STEP 2: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + æ—¥è‹±ç”Ÿæˆï¼ˆæ–°è¦è¨˜äº‹ã®ã¿ï¼‰
# ============================================================

def score_and_translate(articles: list[dict], client: anthropic.Anthropic) -> list[dict]:
    results = []
    for article in articles:
        try:
            prompt = SCORING_PROMPT.format(
                title=article["title"],
                summary=article["summary"],
                source=article["source"],
                trust=article["source_trust"],
                category=article["category"],
            )
            response = client.messages.create(
                model="claude-sonnet-4-6",
                max_tokens=1024,
                messages=[{"role": "user", "content": prompt}]
            )
            scored = parse_json_safe(response.content[0].text.strip())

            if scored.get("importance") == "skip" or scored.get("score", 0) < 60:
                log.info(f"SKIP ({scored.get('score')}): {article['title'][:50]}")
                continue

            article.update(scored)
            article["processed_at"] = datetime.now(timezone.utc).isoformat()
            results.append(article)
            log.info(f"âœ“ [{scored['score']}] {scored['importance'].upper()}: {scored.get('title_ja','')[:40]}")

        except Exception as e:
            log.warning(f"Score error '{article['title'][:40]}': {e}")

    log.info(f"Passed scoring: {len(results)}/{len(articles)}")
    return results

# ============================================================
# STEP 3: Supabaseã«ä¿å­˜
# ============================================================

def save_to_supabase(articles: list[dict], supabase_url: str, supabase_key: str) -> int:
    if not supabase_url or not supabase_key:
        log.warning("Supabase not configured, skipping")
        return 0

    sb = create_client(supabase_url, supabase_key)
    saved = 0
    for article in articles:
        try:
            sb.table("articles").upsert({
                "id":           article["id"],
                "title_ja":     article.get("title_ja", ""),
                "title_en":     article.get("title_en", ""),
                "summary_ja":   article.get("summary_ja", ""),
                "summary_en":   article.get("summary_en", ""),
                "key_insight":  article.get("key_insight", ""),
                "url":          article["url"],
                "source":       article["source"],
                "category":     article["category"],
                "score":        article.get("score", 0),
                "importance":   article.get("importance", "normal"),
                "tags":         article.get("tags", []),
                "processed_at": article.get("processed_at"),
                "status":       DEFAULT_STATUS,
            }, on_conflict="id").execute()
            saved += 1
        except Exception as e:
            log.warning(f"DB error '{article.get('title_ja','')}': {e}")

    log.info(f"Saved {saved}/{len(articles)} articles (status={DEFAULT_STATUS})")
    return saved

# ============================================================
# STEP 4: XæŠ•ç¨¿ï¼ˆpublished ãƒ¢ãƒ¼ãƒ‰ã‹ã¤Xè¨­å®šæ¸ˆã¿ã®ã¿ï¼‰
# ============================================================

def post_to_twitter(articles: list[dict]) -> None:
    if DEFAULT_STATUS == "draft":
        log.info("Draft mode: X posting skipped")
        return

    keys = [os.getenv(k) for k in [
        "TWITTER_API_KEY", "TWITTER_API_SECRET",
        "TWITTER_ACCESS_TOKEN", "TWITTER_ACCESS_SECRET"
    ]]
    if not all(keys):
        log.warning("Twitter credentials not set, skipping")
        return

    try:
        import tweepy
        client = tweepy.Client(
            consumer_key=keys[0], consumer_secret=keys[1],
            access_token=keys[2], access_token_secret=keys[3],
        )
    except ImportError:
        return

    to_post = [a for a in articles if a.get("importance") in ("critical", "high")][:3]
    for a in to_post:
        tags = " ".join(f"#{t.replace(' ','')}" for t in a.get("tags", [])[:3])
        tweet = (
            f"ğŸ”” [{a.get('importance','').upper()}] {a.get('title_ja','')}\n\n"
            f"ğŸ’¡ {a.get('key_insight','')}\n\n"
            f"ğŸ“Š Score: {a.get('score',0)}/100\n"
            f"ğŸ”— {a.get('url','')}\n\n"
            f"{tags} #AINews #SIGNAL"
        )
        if len(tweet) > 280:
            tweet = tweet[:277] + "..."
        try:
            client.create_tweet(text=tweet)
            log.info(f"Tweeted: {a.get('title_ja','')[:40]}")
        except Exception as e:
            log.warning(f"Tweet error: {e}")

# ============================================================
# STEP 5: ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆï¼ˆ1æ—¥1å›ãƒ»UTC0æ™‚å°ã®ã¿ï¼‰
# ============================================================

def generate_daily_digest(articles: list[dict], client: anthropic.Anthropic) -> Optional[str]:
    if not articles:
        return None
    top = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)[:5]
    summaries = "\n".join(
        f"- [{a['source']}] {a.get('title_ja','')}: {a.get('key_insight','')}"
        for a in top
    )
    try:
        response = client.messages.create(
            model="claude-sonnet-4-6",
            max_tokens=800,
            messages=[{"role": "user", "content": f"""
æœ¬æ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒƒãƒ—5ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ç”¨ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
èª­è€…ã¯æŠ€è¡“è€…ã€œãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã¾ã§å¹…åºƒãæƒ³å®šã€‚

{summaries}

ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
- ä»¶åï¼ˆ20å­—ä»¥å†…ï¼‰
- æœ¬æ–‡ï¼ˆ300å­—ç¨‹åº¦ï¼‰
- è‹±èªç‰ˆ
"""}]
        )
        return response.content[0].text
    except Exception as e:
        log.warning(f"Digest failed: {e}")
        return None

# ============================================================
# MAIN
# ============================================================

def run_pipeline():
    log.info("=" * 50)
    log.info(f"SIGNAL Pipeline Started [mode={DEFAULT_STATUS}]")
    log.info("=" * 50)

    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY is required")
    claude = anthropic.Anthropic(api_key=api_key)

    supabase_url = os.getenv("SUPABASE_URL", "")
    supabase_key = os.getenv("SUPABASE_KEY", "")

    # Step 0: æ—¢å­˜IDã‚’å–å¾—ï¼ˆClaudeã‚³ã‚¹ãƒˆã‚’å¤§å¹…å‰Šæ¸›ï¼‰
    existing_ids = get_existing_ids(supabase_url, supabase_key)

    # Step 1: RSSåé›†
    all_articles = fetch_feeds()
    if not all_articles:
        log.error("No articles fetched")
        return

    # æ–°è¦è¨˜äº‹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆâ˜…ã“ã“ãŒã‚³ã‚¹ãƒˆå‰Šæ¸›ã®æ ¸å¿ƒâ˜…ï¼‰
    new_articles = [a for a in all_articles if a["id"] not in existing_ids]
    log.info(f"New articles to process: {len(new_articles)}/{len(all_articles)}")

    if not new_articles:
        log.info("No new articles. Pipeline complete.")
        return

    # Step 2: æ–°è¦ã®ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    scored = score_and_translate(new_articles, claude)
    if not scored:
        log.info("No articles passed scoring threshold")
        return

    # Step 3: DBä¿å­˜
    saved = save_to_supabase(scored, supabase_url, supabase_key)

    # Step 4: XæŠ•ç¨¿
    post_to_twitter(scored)

    # Step 5: 1æ—¥1å›ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆï¼ˆUTC 0æ™‚å°ï¼‰
    if datetime.now(timezone.utc).hour == 0 and saved > 0:
        digest = generate_daily_digest(scored, claude)
        if digest:
            log.info(f"\n{'='*40}\nDAILY DIGEST:\n{digest}\n{'='*40}")

    log.info("=" * 50)
    log.info(f"Done: {saved} new articles saved [{DEFAULT_STATUS}]")
    if DEFAULT_STATUS == "draft":
        log.info(">> Supabase ã§ status ã‚’ 'published' ã«å¤‰æ›´ã™ã‚‹ã¨ã‚µã‚¤ãƒˆã«è¡¨ç¤ºã•ã‚Œã¾ã™")
    for a in sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:5]:
        log.info(f"  [{a.get('score')}] {a.get('importance','').upper()}: {a.get('title_ja','')[:50]}")
    log.info("=" * 50)


if __name__ == "__main__":
    run_pipeline()
