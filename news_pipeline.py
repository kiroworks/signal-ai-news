"""
SIGNAL AI News Pipeline
========================
è‡ªå‹•ã§AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›† â†’ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â†’ æ—¥è‹±è¨˜äº‹ç”Ÿæˆ â†’ DBä¿å­˜(draft) â†’ XæŠ•ç¨¿

å¿…è¦ãªç’°å¢ƒå¤‰æ•°ï¼ˆGitHub Secretsï¼‰:
  ANTHROPIC_API_KEY    = Claude APIã‚­ãƒ¼
  SUPABASE_URL         = Supabaseãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆURL
  SUPABASE_KEY         = Supabase service_role keyï¼ˆSecret keyï¼‰

ä»»æ„ï¼ˆXé€£æºï¼‰:
  TWITTER_API_KEY      = X API Key
  TWITTER_API_SECRET   = X API Secret
  TWITTER_ACCESS_TOKEN = X Access Token
  TWITTER_ACCESS_SECRET= X Access Token Secret

å®Ÿè¡Œ:
  pip install anthropic feedparser supabase tweepy python-dotenv
  python news_pipeline.py
"""

import os
import re
import json
import hashlib
import logging
from datetime import datetime, timezone, timedelta
from typing import Optional
import feedparser
import anthropic
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
log = logging.getLogger("signal")

# ============================================================
# CONFIG
# ============================================================

# è¨˜äº‹ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
# "draft"   = Supabaseã§ç¢ºèªå¾Œã«æ‰‹å‹•ã§publishedã«å¤‰æ›´ï¼ˆæ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰
# "published" = å³æ™‚å…¬é–‹ï¼ˆå…¨è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ï¼‰
DEFAULT_STATUS = "draft"

# å³é¸ã‚½ãƒ¼ã‚¹ä¸€è¦§ï¼ˆä¿¡é ¼æ€§ãƒ»å°‚é–€æ€§ã®é«˜ã„ã‚‚ã®ã ã‘ï¼‰
RSS_SOURCES = [
    # ç ”ç©¶ãƒ»è«–æ–‡
    {"url": "https://arxiv.org/rss/cs.AI",           "source": "arXiv AI",        "category": "research", "trust": 95},
    {"url": "https://arxiv.org/rss/cs.LG",           "source": "arXiv ML",        "category": "research", "trust": 95},
    {"url": "https://deepmind.google/blog/rss.xml",  "source": "DeepMind",        "category": "research", "trust": 98},

    # å…¬å¼ãƒ–ãƒ­ã‚°
    {"url": "https://openai.com/blog/rss.xml",       "source": "OpenAI",          "category": "product",  "trust": 99},
    {"url": "https://www.anthropic.com/rss.xml",     "source": "Anthropic",       "category": "product",  "trust": 99},
    {"url": "https://ai.google/blog/rss",            "source": "Google AI",       "category": "product",  "trust": 97},
    {"url": "https://ai.meta.com/blog/rss",          "source": "Meta AI",         "category": "product",  "trust": 96},
    {"url": "https://mistral.ai/news/rss.xml",       "source": "Mistral AI",      "category": "product",  "trust": 92},

    # ãƒ†ãƒƒã‚¯ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆé«˜å“è³ªã®ã‚‚ã®ã®ã¿ï¼‰
    {"url": "https://techcrunch.com/tag/artificial-intelligence/feed/",
                                                     "source": "TechCrunch",      "category": "business", "trust": 80},
    {"url": "https://www.technologyreview.com/feed/","source": "MIT Tech Review", "category": "research", "trust": 90},
    {"url": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
                                                     "source": "The Verge",       "category": "product",  "trust": 78},

    # ãƒãƒªã‚·ãƒ¼ãƒ»è¦åˆ¶
    {"url": "https://artificialintelligenceact.eu/feed/",
                                                     "source": "EU AI Act",       "category": "policy",   "trust": 95},
    {"url": "https://www.nist.gov/artificial-intelligence/rss.xml",
                                                     "source": "NIST",            "category": "policy",   "trust": 97},
]

SCORING_CRITERIA = """
ä»¥ä¸‹ã®åŸºæº–ã§0-100ã®ã‚¹ã‚³ã‚¢ã‚’ã¤ã‘ã¦ãã ã•ã„ï¼š
- æ–°è¦æ€§: æ—¢çŸ¥ã®æƒ…å ±ã§ã¯ãªãæ–°ã—ã„ç™ºè¦‹ãƒ»ç™ºè¡¨ã‹ (30ç‚¹)
- ä¿¡é ¼æ€§: ä¸€æ¬¡ã‚½ãƒ¼ã‚¹ãƒ»æŸ»èª­æ¸ˆã¿ãƒ»å…¬å¼ç™ºè¡¨ã‹ (25ç‚¹)
- é‡è¦æ€§: AIåˆ†é‡å…¨ä½“ã«å½±éŸ¿ã™ã‚‹ã‹ (25ç‚¹)
- å®Ÿç”¨æ€§: é–‹ç™ºè€…ãƒ»ç ”ç©¶è€…ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã«æœ‰ç›Šã‹ (20ç‚¹)

ã‚¹ã‚³ã‚¢åŸºæº–:
90+ = CRITICAL: å³åº§ã«æ³¨ç›®ã™ã¹ãé‡å¤§ãƒ‹ãƒ¥ãƒ¼ã‚¹
75-89 = HIGH: é‡è¦ãªã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã€åºƒãå…±æœ‰ã™ã‚‹ä¾¡å€¤ã‚ã‚Š
60-74 = NORMAL: å‚è€ƒã«ãªã‚‹æƒ…å ±
60æœªæº€ = SKIP: æ²è¼‰ã—ãªã„
"""

# ============================================================
# UTILITIES
# ============================================================

def strip_html(text: str) -> str:
    """HTMLã‚¿ã‚°ã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
    text = re.sub(r'<[^>]+>', '', text)
    text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>') \
               .replace('&quot;', '"').replace('&#39;', "'").replace('&nbsp;', ' ')
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def is_today(date_str: str) -> bool:
    """è¨˜äº‹ãŒä»Šæ—¥ï¼ˆUTCï¼‰ã®ã‚‚ã®ã‹ã©ã†ã‹åˆ¤å®š"""
    try:
        today = datetime.now(timezone.utc).date()
        dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        return dt.date() == today
    except Exception:
        return False

# ============================================================
# PIPELINE STEPS
# ============================================================

def fetch_feeds() -> list[dict]:
    """Step 1: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’åé›†"""
    articles = []
    seen_urls = set()

    for source in RSS_SOURCES:
        try:
            feed = feedparser.parse(source["url"])
            count = 0
            for entry in feed.entries[:5]:  # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰æœ€å¤§5ä»¶
                url = entry.get("link", "")
                if not url or url in seen_urls:
                    continue
                seen_urls.add(url)

                article_id = hashlib.md5(url.encode()).hexdigest()
                raw_summary = entry.get("summary", entry.get("description", ""))

                articles.append({
                    "id": article_id,
                    "title": strip_html(entry.get("title", "")),
                    "summary": strip_html(raw_summary)[:2000],
                    "url": url,
                    "source": source["source"],
                    "source_trust": source["trust"],
                    "category": source["category"],
                    "published": entry.get("published", datetime.now(timezone.utc).isoformat()),
                })
                count += 1
            log.info(f"âœ“ {source['source']}: {count} entries")
        except Exception as e:
            log.warning(f"âœ— {source['source']}: {e}")

    log.info(f"Total fetched: {len(articles)} articles")
    return articles


def score_and_translate(articles: list[dict], client: anthropic.Anthropic) -> list[dict]:
    """Step 2: Claude APIã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + æ—¥è‹±è¨˜äº‹ç”Ÿæˆ"""
    results = []
    for article in articles:
        try:
            prompt = f"""
ã‚ãªãŸã¯AIå°‚é–€ã®ã‚¸ãƒ£ãƒ¼ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è¨˜äº‹ã‚’è©•ä¾¡ãƒ»ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘{article['title']}
ã€æœ¬æ–‡è¦ç´„ã€‘{article['summary']}
ã€ã‚½ãƒ¼ã‚¹ã€‘{article['source']} (ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {article['source_trust']}/100)
ã€ã‚«ãƒ†ã‚´ãƒªã€‘{article['category']}

{SCORING_CRITERIA}

ä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆJSONã®ã¿ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ä¸è¦ï¼‰:
{{
  "score": <0-100ã®æ•´æ•°>,
  "importance": <"critical"|"high"|"normal"|"skip">,
  "title_ja": <æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ30å­—ä»¥å†…ï¼‰>,
  "title_en": <è‹±èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ50 wordsä»¥å†…ï¼‰>,
  "summary_ja": <æ—¥æœ¬èªè¦ç´„ï¼ˆ120å­—ç¨‹åº¦ï¼‰>,
  "summary_en": <è‹±èªè¦ç´„ï¼ˆ100 wordsç¨‹åº¦ï¼‰>,
  "tags": <é–¢é€£ã‚¿ã‚°ã®é…åˆ— ä¾‹: ["LLM", "OpenAI", "Benchmark"]>,
  "key_insight": <ã“ã®è¨˜äº‹ã®æœ€é‡è¦ãƒã‚¤ãƒ³ãƒˆ1è¡Œï¼ˆæ—¥æœ¬èªï¼‰>
}}
"""
            response = client.messages.create(
                model="claude-sonnet-4-6",
                max_tokens=1024,
                messages=[{"role": "user", "content": prompt}]
            )
            raw = response.content[0].text.strip()

            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’å®‰å…¨ã«ãƒ‘ãƒ¼ã‚¹
            if "```" in raw:
                raw = raw.split("```")[1].replace("json", "", 1).strip()
            # å…ˆé ­ã® { ã‚’èµ·ç‚¹ã«ãƒ‘ãƒ¼ã‚¹ï¼ˆå‰å¾Œã®ã‚´ãƒŸã‚’é™¤å»ï¼‰
            start = raw.find("{")
            end = raw.rfind("}") + 1
            if start == -1 or end == 0:
                raise ValueError("JSON not found in response")
            scored = json.loads(raw[start:end])

            if scored.get("importance") == "skip" or scored.get("score", 0) < 60:
                log.info(f"SKIP (score={scored.get('score')}): {article['title'][:50]}")
                continue

            article.update(scored)
            article["processed_at"] = datetime.now(timezone.utc).isoformat()
            results.append(article)
            log.info(f"âœ“ score={scored['score']} [{scored['importance'].upper()}]: {scored['title_ja'][:40]}")

        except Exception as e:
            log.warning(f"Score error for '{article['title'][:40]}': {e}")

    log.info(f"Passed scoring: {len(results)}/{len(articles)} articles")
    return results


def save_to_supabase(articles: list[dict], supabase_url: str, supabase_key: str) -> int:
    """Step 3: Supabaseã«ä¿å­˜ï¼ˆé‡è¤‡ã‚¹ã‚­ãƒƒãƒ—ï¼‰"""
    if not supabase_url or not supabase_key:
        log.warning("Supabase not configured, skipping DB save")
        return 0

    sb = create_client(supabase_url, supabase_key)
    saved = 0
    for article in articles:
        try:
            sb.table("articles").upsert({
                "id":          article["id"],
                "title_ja":    article.get("title_ja", ""),
                "title_en":    article.get("title_en", ""),
                "summary_ja":  article.get("summary_ja", ""),
                "summary_en":  article.get("summary_en", ""),
                "key_insight": article.get("key_insight", ""),
                "url":         article["url"],
                "source":      article["source"],
                "category":    article["category"],
                "score":       article.get("score", 0),
                "importance":  article.get("importance", "normal"),
                "tags":        article.get("tags", []),
                "processed_at":article.get("processed_at"),
                "status":      DEFAULT_STATUS,      # draft or published
            }, on_conflict="id").execute()
            saved += 1
        except Exception as e:
            log.warning(f"DB error for '{article.get('title_ja', '')}': {e}")

    log.info(f"Saved {saved}/{len(articles)} articles to Supabase (status={DEFAULT_STATUS})")
    return saved


def post_to_twitter(articles: list[dict]) -> None:
    """Step 4: é‡è¦è¨˜äº‹ã®ã¿Xã«æŠ•ç¨¿ï¼ˆCRITICAL/HIGH ã‹ã¤ published ã®ã¿ï¼‰"""
    # DEFAULT_STATUS ãŒ draft ã®å ´åˆã¯XæŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœªãƒ¬ãƒ“ãƒ¥ãƒ¼è¨˜äº‹ã¯æŠ•ç¨¿ã—ãªã„ï¼‰
    if DEFAULT_STATUS == "draft":
        log.info("Draft mode: X posting skipped (review articles in Supabase first)")
        return

    keys = [
        os.getenv("TWITTER_API_KEY"),
        os.getenv("TWITTER_API_SECRET"),
        os.getenv("TWITTER_ACCESS_TOKEN"),
        os.getenv("TWITTER_ACCESS_SECRET"),
    ]
    if not all(keys):
        log.warning("Twitter credentials not configured, skipping posts")
        return

    try:
        import tweepy
        client = tweepy.Client(
            consumer_key=keys[0],
            consumer_secret=keys[1],
            access_token=keys[2],
            access_token_secret=keys[3],
        )
    except ImportError:
        log.warning("tweepy not installed, skipping X post")
        return

    to_post = [a for a in articles if a.get("importance") in ("critical", "high")][:3]

    for article in to_post:
        score      = article.get("score", 0)
        importance = article.get("importance", "").upper()
        key_insight= article.get("key_insight", "")
        title      = article.get("title_ja", "")
        url        = article.get("url", "")
        tags       = " ".join([f"#{t.replace(' ', '')}" for t in article.get("tags", [])[:3]])

        tweet = f"ğŸ”” [{importance}] {title}\n\n{key_insight}\n\nğŸ“Š Score: {score}/100\nğŸ”— {url}\n\n{tags} #AINews #SIGNAL"

        if len(tweet) > 280:
            tweet = tweet[:277] + "..."

        try:
            client.create_tweet(text=tweet)
            log.info(f"âœ“ Tweeted: {title[:40]}")
        except Exception as e:
            log.warning(f"Tweet error: {e}")


def generate_daily_digest(articles: list[dict], client: anthropic.Anthropic) -> Optional[str]:
    """Step 5: ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆç”Ÿæˆï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ç”¨ãƒ»1æ—¥1å›ã®å®Ÿè¡Œæ™‚ã®ã¿ï¼‰"""
    if not articles:
        return None

    top = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)[:5]
    summaries = "\n".join([
        f"- [{a['source']}] {a.get('title_ja', '')}: {a.get('key_insight', '')}"
        for a in top
    ])

    try:
        response = client.messages.create(
            model="claude-sonnet-4-6",
            max_tokens=800,
            messages=[{
                "role": "user",
                "content": f"""ä»¥ä¸‹ã®æœ¬æ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒƒãƒ—5ã‹ã‚‰ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ç”¨ã®ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
èª­è€…ã¯æŠ€è¡“è€…ã‹ã‚‰ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã¾ã§å¹…åºƒãæƒ³å®šã€‚ç°¡æ½”ã‹ã¤æ´å¯Ÿã«å¯Œã‚“ã å†…å®¹ã§ã€‚

{summaries}

ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
- ä»¶åï¼ˆ20å­—ä»¥å†…ï¼‰
- æœ¬æ–‡ï¼ˆ300å­—ç¨‹åº¦ã€ç®‡æ¡æ›¸ãå¯ï¼‰
- è‹±èªç‰ˆã‚‚è¿½è¨˜
"""
            }]
        )
        digest = response.content[0].text
        log.info("Daily digest generated")
        return digest
    except Exception as e:
        log.warning(f"Digest generation failed: {e}")
        return None


# ============================================================
# MAIN
# ============================================================

def run_pipeline():
    log.info("=" * 50)
    log.info(f"SIGNAL Pipeline Started  [mode={DEFAULT_STATUS}]")
    log.info("=" * 50)

    # Init Claude
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY is required")
    claude = anthropic.Anthropic(api_key=api_key)

    # Step 1: Fetch
    articles = fetch_feeds()
    if not articles:
        log.error("No articles fetched")
        return

    # Step 2: Score & Translate
    scored = score_and_translate(articles, claude)
    if not scored:
        log.info("No articles passed scoring threshold")
        return

    # Step 3: Save to DB
    saved_count = save_to_supabase(
        scored,
        os.getenv("SUPABASE_URL", ""),
        os.getenv("SUPABASE_KEY", "")
    )

    # Step 4: Post to Xï¼ˆdraft ãƒ¢ãƒ¼ãƒ‰ã§ã¯è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    post_to_twitter(scored)

    # Step 5: Daily digestï¼ˆUTCã§0æ™‚å°ã®å®Ÿè¡Œæ™‚ã®ã¿ç”Ÿæˆï¼‰
    current_hour = datetime.now(timezone.utc).hour
    if current_hour == 0 and saved_count > 0:
        digest = generate_daily_digest(scored, claude)
        if digest:
            log.info(f"\n{'='*40}\nDAILY DIGEST:\n{digest}\n{'='*40}")

    # Summary
    log.info("=" * 50)
    log.info(f"Pipeline complete: {saved_count} articles saved as [{DEFAULT_STATUS}]")
    if DEFAULT_STATUS == "draft":
        log.info(">> Supabase Table Editor ã§è¨˜äº‹ã‚’ç¢ºèªã—ã€status ã‚’ 'published' ã«å¤‰æ›´ã—ã¦ãã ã•ã„")
    for a in sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:5]:
        log.info(f"  [{a.get('score')}] {a.get('importance','').upper()}: {a.get('title_ja', '')[:50]}")
    log.info("=" * 50)


if __name__ == "__main__":
    run_pipeline()
