"""
SIGNAL AI News Pipeline
========================
è‡ªå‹•ã§AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›† â†’ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â†’ æ—¥è‹±è¨˜äº‹ç”Ÿæˆ â†’ DBä¿å­˜ â†’ XæŠ•ç¨¿

å¿…è¦ãªç’°å¢ƒå¤‰æ•°:
  ANTHROPIC_API_KEY    = Claude APIã‚­ãƒ¼
  SUPABASE_URL         = Supabaseãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆURL
  SUPABASE_KEY         = Supabase anon key
  TWITTER_BEARER       = X(Twitter) Bearer Token
  TWITTER_API_KEY      = X API Key
  TWITTER_API_SECRET   = X API Secret
  TWITTER_ACCESS_TOKEN = X Access Token
  TWITTER_ACCESS_SECRET= X Access Token Secret

å®Ÿè¡Œ:
  pip install anthropic feedparser supabase tweepy python-dotenv
  python news_pipeline.py
"""

import os
import json
import hashlib
import logging
from datetime import datetime, timezone
from typing import Optional
import feedparser
import anthropic
from supabase import create_client
import tweepy
from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
log = logging.getLogger("signal")

# ============================================================
# CONFIG
# ============================================================

# å³é¸ã‚½ãƒ¼ã‚¹ä¸€è¦§ï¼ˆä¿¡é ¼æ€§ãƒ»å°‚é–€æ€§ã®é«˜ã„ã‚‚ã®ã ã‘ï¼‰
RSS_SOURCES = [
    # ç ”ç©¶ãƒ»è«–æ–‡
    {"url": "https://arxiv.org/rss/cs.AI",          "source": "arXiv AI",         "category": "research", "trust": 95},
    {"url": "https://arxiv.org/rss/cs.LG",          "source": "arXiv ML",         "category": "research", "trust": 95},
    {"url": "https://deepmind.google/blog/rss.xml", "source": "DeepMind",         "category": "research", "trust": 98},

    # å…¬å¼ãƒ–ãƒ­ã‚°
    {"url": "https://openai.com/blog/rss.xml",      "source": "OpenAI",           "category": "product",  "trust": 99},
    {"url": "https://www.anthropic.com/rss.xml",    "source": "Anthropic",        "category": "product",  "trust": 99},
    {"url": "https://ai.google/blog/rss",           "source": "Google AI",        "category": "product",  "trust": 97},
    {"url": "https://ai.meta.com/blog/rss",         "source": "Meta AI",          "category": "product",  "trust": 96},
    {"url": "https://mistral.ai/news/rss.xml",      "source": "Mistral AI",       "category": "product",  "trust": 92},

    # ãƒ†ãƒƒã‚¯ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆé«˜å“è³ªã®ã‚‚ã®ã®ã¿ï¼‰
    {"url": "https://techcrunch.com/tag/artificial-intelligence/feed/",
                                                    "source": "TechCrunch",       "category": "business", "trust": 80},
    {"url": "https://www.technologyreview.com/feed/","source": "MIT Tech Review", "category": "research", "trust": 90},
    {"url": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
                                                    "source": "The Verge",        "category": "product",  "trust": 78},

    # ãƒãƒªã‚·ãƒ¼ãƒ»è¦åˆ¶
    {"url": "https://artificialintelligenceact.eu/feed/",
                                                    "source": "EU AI Act",        "category": "policy",   "trust": 95},
    {"url": "https://www.nist.gov/artificial-intelligence/rss.xml",
                                                    "source": "NIST",             "category": "policy",   "trust": 97},
]

# ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–ï¼ˆClaude promptã«æ¸¡ã™ï¼‰
SCORING_CRITERIA = """
ä»¥ä¸‹ã®åŸºæº–ã§0-100ã®ã‚¹ã‚³ã‚¢ã‚’ã¤ã‘ã¦ãã ã•ã„ï¼š
- æ–°è¦æ€§: æ—¢çŸ¥ã®æƒ…å ±ã§ã¯ãªãæ–°ã—ã„ç™ºè¦‹ãƒ»ç™ºè¡¨ã‹ (30ç‚¹)
- ä¿¡é ¼æ€§: ä¸€æ¬¡ã‚½ãƒ¼ã‚¹ãƒ»æŸ»èª­æ¸ˆã¿ãƒ»å…¬å¼ç™ºè¡¨ã‹ (25ç‚¹)
- é‡è¦æ€§: AIåˆ†é‡å…¨ä½“ã«å½±éŸ¿ã™ã‚‹ã‹ (25ç‚¹)
- å®Ÿç”¨æ€§: é–‹ç™ºè€…ãƒ»ç ”ç©¶è€…ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã«æœ‰ç›Šã‹ (20ç‚¹)

ã‚¹ã‚³ã‚¢åŸºæº–:
90+ = CRITICAL: å³åº§ã«æ³¨ç›®ã™ã¹ãé‡å¤§ãƒ‹ãƒ¥ãƒ¼ã‚¹
75-89 = HIGH: é‡è¦ãªã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã€åºƒãå…±æœ‰ã™ã‚‹ä¾¡å€¤ã‚ã‚Š
60-74 = NORMAL: å‚è€ƒã«ãªã‚‹æƒ…å ±
60æœªæº€ = SKIP: æ²è¼‰ã—ãªã„
"""

# ============================================================
# PIPELINE STEPS
# ============================================================

def fetch_feeds() -> list[dict]:
    """Step 1: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’åé›†"""
    articles = []
    for source in RSS_SOURCES:
        try:
            feed = feedparser.parse(source["url"])
            for entry in feed.entries[:5]:  # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰æœ€å¤§5ä»¶
                article_id = hashlib.md5(entry.get("link", "").encode()).hexdigest()
                articles.append({
                    "id": article_id,
                    "title": entry.get("title", ""),
                    "summary": entry.get("summary", entry.get("description", ""))[:2000],
                    "url": entry.get("link", ""),
                    "source": source["source"],
                    "source_trust": source["trust"],
                    "category": source["category"],
                    "published": entry.get("published", datetime.now(timezone.utc).isoformat()),
                })
            log.info(f"âœ“ {source['source']}: {len(feed.entries)} entries")
        except Exception as e:
            log.warning(f"âœ— {source['source']}: {e}")
    log.info(f"Total fetched: {len(articles)} articles")
    return articles


def score_and_translate(articles: list[dict], client: anthropic.Anthropic) -> list[dict]:
    """Step 2: Claude APIã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + æ—¥è‹±è¨˜äº‹ç”Ÿæˆ"""
    results = []
    for article in articles:
        try:
            prompt = f"""
ã‚ãªãŸã¯AIå°‚é–€ã®ã‚¸ãƒ£ãƒ¼ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è¨˜äº‹ã‚’è©•ä¾¡ãƒ»ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘{article['title']}
ã€æœ¬æ–‡è¦ç´„ã€‘{article['summary']}
ã€ã‚½ãƒ¼ã‚¹ã€‘{article['source']} (ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {article['source_trust']}/100)
ã€ã‚«ãƒ†ã‚´ãƒªã€‘{article['category']}

{SCORING_CRITERIA}

ä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆJSONã®ã¿ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ä¸è¦ï¼‰:
{{
  "score": <0-100ã®æ•´æ•°>,
  "importance": <"critical"|"high"|"normal"|"skip">,
  "title_ja": <æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ30å­—ä»¥å†…ï¼‰>,
  "title_en": <è‹±èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ50 wordsä»¥å†…ï¼‰>,
  "summary_ja": <æ—¥æœ¬èªè¦ç´„ï¼ˆ120å­—ç¨‹åº¦ï¼‰>,
  "summary_en": <è‹±èªè¦ç´„ï¼ˆ100 wordsç¨‹åº¦ï¼‰>,
  "tags": <é–¢é€£ã‚¿ã‚°ã®é…åˆ— ä¾‹: ["LLM", "OpenAI", "Benchmark"]>,
  "key_insight": <ã“ã®è¨˜äº‹ã®æœ€é‡è¦ãƒã‚¤ãƒ³ãƒˆ1è¡Œï¼ˆæ—¥æœ¬èªï¼‰>
}}
"""
            response = client.messages.create(
                model="claude-sonnet-4-6",
                max_tokens=1024,
                messages=[{"role": "user", "content": prompt}]
            )
            raw = response.content[0].text.strip()
            # JSONã‚’å®‰å…¨ã«ãƒ‘ãƒ¼ã‚¹
            if "```" in raw:
                raw = raw.split("```")[1].replace("json", "").strip()
            scored = json.loads(raw)

            if scored.get("importance") == "skip" or scored.get("score", 0) < 60:
                log.info(f"SKIP (score={scored.get('score')}): {article['title'][:50]}")
                continue

            article.update(scored)
            article["processed_at"] = datetime.now(timezone.utc).isoformat()
            results.append(article)
            log.info(f"âœ“ score={scored['score']} [{scored['importance']}]: {scored['title_ja'][:40]}")

        except Exception as e:
            log.warning(f"Score error for {article['title'][:40]}: {e}")
    log.info(f"Passed scoring: {len(results)}/{len(articles)} articles")
    return results


def save_to_supabase(articles: list[dict], supabase_url: str, supabase_key: str) -> None:
    """Step 3: Supabaseã«ä¿å­˜ï¼ˆé‡è¤‡ã‚¹ã‚­ãƒƒãƒ—ï¼‰"""
    if not supabase_url or not supabase_key:
        log.warning("Supabase not configured, skipping DB save")
        return

    sb = create_client(supabase_url, supabase_key)
    for article in articles:
        try:
            sb.table("articles").upsert({
                "id": article["id"],
                "title_ja": article.get("title_ja", ""),
                "title_en": article.get("title_en", ""),
                "summary_ja": article.get("summary_ja", ""),
                "summary_en": article.get("summary_en", ""),
                "key_insight": article.get("key_insight", ""),
                "url": article["url"],
                "source": article["source"],
                "category": article["category"],
                "score": article.get("score", 0),
                "importance": article.get("importance", "normal"),
                "tags": article.get("tags", []),
                "processed_at": article.get("processed_at"),
            }, on_conflict="id").execute()
        except Exception as e:
            log.warning(f"DB error for {article.get('title_ja', '')}: {e}")
    log.info(f"Saved {len(articles)} articles to Supabase")


def post_to_twitter(articles: list[dict]) -> None:
    """Step 4: é‡è¦è¨˜äº‹ã®ã¿Xã«æŠ•ç¨¿"""
    keys = [
        os.getenv("TWITTER_API_KEY"),
        os.getenv("TWITTER_API_SECRET"),
        os.getenv("TWITTER_ACCESS_TOKEN"),
        os.getenv("TWITTER_ACCESS_SECRET"),
    ]
    if not all(keys):
        log.warning("Twitter not configured, skipping posts")
        return

    client = tweepy.Client(
        consumer_key=keys[0],
        consumer_secret=keys[1],
        access_token=keys[2],
        access_token_secret=keys[3],
    )

    # CRITICALã¨HIGHã®ã¿æŠ•ç¨¿ï¼ˆã‚¹ãƒ‘ãƒ é˜²æ­¢ï¼‰
    to_post = [a for a in articles if a.get("importance") in ("critical", "high")][:3]

    for article in to_post:
        score = article.get("score", 0)
        importance = article.get("importance", "").upper()
        key_insight = article.get("key_insight", "")
        title = article.get("title_ja", "")
        url = article.get("url", "")
        tags = " ".join([f"#{t.replace(' ','')}" for t in article.get("tags", [])[:3]])

        tweet = f"""ğŸ”” [{importance}] {title}

{key_insight}

ğŸ“Š Quality Score: {score}/100
ğŸ”— {url}

{tags} #AINews #SIGNAL"""

        # 280æ–‡å­—åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if len(tweet) > 280:
            tweet = tweet[:277] + "..."

        try:
            client.create_tweet(text=tweet)
            log.info(f"âœ“ Tweeted: {title[:40]}")
        except Exception as e:
            log.warning(f"Tweet error: {e}")


def generate_daily_digest(articles: list[dict], client: anthropic.Anthropic) -> str:
    """ãƒœãƒ¼ãƒŠã‚¹: 1æ—¥1å›ã®ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆç”Ÿæˆï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ç”¨ï¼‰"""
    top_articles = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)[:5]
    summaries = "\n".join([f"- [{a['source']}] {a.get('title_ja', '')}: {a.get('key_insight', '')}"
                           for a in top_articles])

    response = client.messages.create(
        model="claude-sonnet-4-6",
        max_tokens=800,
        messages=[{
            "role": "user",
            "content": f"""ä»¥ä¸‹ã®æœ¬æ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒƒãƒ—5ã‹ã‚‰ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ç”¨ã®ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
èª­è€…ã¯æŠ€è¡“è€…ã‹ã‚‰ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã¾ã§å¹…åºƒãæƒ³å®šã€‚ç°¡æ½”ã‹ã¤æ´å¯Ÿã«å¯Œã‚“ã å†…å®¹ã§ã€‚

{summaries}

ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: 
- ä»¶åï¼ˆ20å­—ä»¥å†…ï¼‰
- æœ¬æ–‡ï¼ˆ300å­—ç¨‹åº¦ã€ç®‡æ¡æ›¸ãå¯ï¼‰
- è‹±èªç‰ˆã‚‚è¿½è¨˜
"""
        }]
    )
    return response.content[0].text


# ============================================================
# MAIN
# ============================================================

def run_pipeline():
    log.info("=" * 50)
    log.info("SIGNAL Pipeline Started")
    log.info("=" * 50)

    # Init Claude
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY is required")
    claude = anthropic.Anthropic(api_key=api_key)

    # Step 1: Fetch
    articles = fetch_feeds()
    if not articles:
        log.error("No articles fetched")
        return

    # Step 2: Score & Translate
    scored = score_and_translate(articles, claude)
    if not scored:
        log.info("No articles passed scoring threshold")
        return

    # Step 3: Save to DB
    save_to_supabase(
        scored,
        os.getenv("SUPABASE_URL", ""),
        os.getenv("SUPABASE_KEY", "")
    )

    # Step 4: Post to X
    post_to_twitter(scored)

    # Summary
    log.info("=" * 50)
    log.info(f"Pipeline complete: {len(scored)} articles published")
    for a in sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:5]:
        log.info(f"  [{a.get('score')}] {a.get('title_ja', '')[:50]}")
    log.info("=" * 50)


if __name__ == "__main__":
    run_pipeline()
